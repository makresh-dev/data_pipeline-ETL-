# ğŸ“˜ Data Pipeline (PySpark) Overview 
A fully functional, scalable ETL data pipeline built using PySpark, supporting:

    - Modular plug-able transformations

    - Multi-format outputs (JSON, Parquet, gzip)

    - Connector-based design (file system, extendable to S3/GCP/SQL)

    - Docker packaging

    - GitHub Actions CI/CD

    - Clean folder structure

    - Easy local + container execution

This matches the typical structure used in production-grade data engineering systems.

# ğŸ“‚ Project Structure
```bash
data_pipeline/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ sales.json
â”‚   â””â”€â”€ region_targets.json
â”‚
â”œâ”€â”€ output/                # Auto generated by pipeline
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ pipeline.py
â”‚   â””â”€â”€ transformations/
â”‚       â”œâ”€â”€ group_by.py
â”‚       â””â”€â”€ join_data.py
â”‚
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

# ğŸ§  Full Data Pipeline Structure
```mermaid
flowchart TD;

    subgraph InputData[ğŸ“¥ Input Data Sources]
        A[sales.json]
        B[region_targets.json]
    end

    subgraph ETL[âš™ï¸ PySpark ETL Pipeline]
        C[Read Input Files<br>spark.read.json]
        D[Transformation 1:<br>GroupBy Sales]
        E[Transformation 2:<br>Join with Targets]
        F[Final DataFrame]
    end

    subgraph OutputData[ğŸ“¤ Output Formats]
        G[JSON Output<br>/output/final_json]
        H[Parquet Output<br>/output/final_parquet]
        I[gzip JSON Output<br>/output/final_gzip]
    end

    A --> C
    B --> C
    C --> D --> E --> F
    F --> G
    F --> H
    F --> I
```


# ğŸ”§ Features

### âœ” Scalable PySpark Pipeline
### âœ” Plug-able Transformations

* GroupBy
* Join

### âœ” Connector-Based Read/Write
Uses file connectors (local filesystem), but paths can easily be swapped to:

* S3
* GCP
* Hadoop
* SQL JDBC

### âœ” Multi Format Outputs

* JSON
* Parquet
* gzip-compressed JSON

### âœ” Dockerized
Fully containerized & environment-agnostic.

### âœ” CI/CD Included
GitHub Actions workflow runs the ETL pipeline automatically.

# ğŸ“¥ Input Data
`data/sales.json`
```json
[
  {"region": "North", "product": "A", "sales": 100},
  {"region": "South", "product": "A", "sales": 150},
  {"region": "North", "product": "B", "sales": 200},
  {"region": "East", "product": "A", "sales": 120}
]
```

`data/region_targets.json`
```json
[
  {"region": "North", "target": 250},
  {"region": "South", "target": 200},
  {"region": "East", "target": 150}
]
```

# ğŸ§© Transformations
GroupBy Transformation (`group_by.py`)
```python
from pyspark.sql import functions as F

def group_by_sales(df):
    return df.groupBy("region").agg(F.sum("sales").alias("total_sales"))

Join Transformation (join_data.py)
def join_targets(sales_df, target_df):
    return sales_df.join(target_df, on="region", how="left")
```

# ğŸ Pipeline Execution
`pipeline.py` creates:

* grouped totals
* merged with target table
* exported in 3 formats

Output is stored in:
```bash
output/
    final_json/
    final_parquet/
    final_gzip/
```

# ğŸ–¥ï¸  Running Locally
Install dependencies:
```python
pip install -r requirements.txt
```
Run pipeline:
```python
python src/pipeline.py
```

# ğŸ³  Running with Docker
Build image:
```bash
docker build -t data-pipeline .
```
Run container:
```
docker run data-pipeline
```


Outputs will be created inside containerâ€™s `/app/output`.

# ğŸ” CI/CD (GitHub Actions)
Workflows Included:

*ETL Pipeline Test*
Automatically:
* installs Java + Python
* installs PySpark
* runs pipeline.py
* lists output directory

*Optional Docker Build & Push*
Builds multi-architecture image
Publishes to Docker Hub (if secrets configured)

# âœ¨ Extending the Pipeline

You can easily add:
* âœ” new transformations
* âœ” new connectors (S3, SQL, GCS)
* âœ” new output formats
* âœ” scheduling via Airflow
* âœ” logging & monitoring

This structure follows the standard modular ETL architecture used in production.

[![ETL Pipeline Test](https://github.com/makresh-dev/data_pipeline-ETL-/actions/workflows/etl-test.yml/badge.svg)](https://github.com/makresh-dev/data_pipeline-ETL-/actions/workflows/docker-build-and-push.yml) ![Python Version](https://img.shields.io/badge/python-3.10-blue) ![Last Commit](https://img.shields.io/github/last-commit/makresh-dev/data_pipeline-ETL-) ![License](https://img.shields.io/badge/license-MIT-green) ![Issues](https://img.shields.io/github/issues/makresh-dev/data_pipeline-ETL-)
